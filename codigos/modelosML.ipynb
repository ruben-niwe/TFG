{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Cambiar el directorio de trabajo\n",
        "%cd '/content/drive/My Drive/tfg/tfg/'\n",
        "\n",
        "# Mostrar el directorio actual para confirmar el cambio\n",
        "directorio_actual = %pwd\n",
        "print(\"Directorio actual:\", directorio_actual)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "84NItbGo0X_X",
        "outputId": "5b140ca4-b1fc-4be3-b2d5-7c71beca90bd"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "/content/drive/My Drive/tfg/tfg\n",
            "Directorio actual: /content/drive/My Drive/tfg/tfg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "path_actual = os.getcwd()\n",
        "subdirectorio = 'datas'\n",
        "file_train = 'df_train.csv'\n",
        "path_train = os.path.join(path_actual, subdirectorio, file_train)"
      ],
      "metadata": {
        "id": "NKPJLKFE06eq"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_train = pd.read_csv(path_train, index_col=None)\n"
      ],
      "metadata": {
        "id": "BR469VEF1Bom"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = df_train.drop(['trueE'],axis=1)"
      ],
      "metadata": {
        "id": "VLk5EQ5E1GJ9"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def filtrado_datos_concatenados(df, N):\n",
        "    # Filtrado inicial por PDGcode\n",
        "    codes = df['PDGcode'].unique()\n",
        "    kaones = []\n",
        "    labels = []\n",
        "\n",
        "    df_filtrado = df[df['PDGcode'].isin(codes)]\n",
        "\n",
        "    # Identificar todos los eventIDs únicos\n",
        "    unique_eventIDs = df_filtrado['eventID'].unique()\n",
        "\n",
        "    # Iterar sobre cada eventID único\n",
        "    for eventID in unique_eventIDs:\n",
        "        # Filtrando el DataFrame por eventID\n",
        "        df_evento = df_filtrado[df_filtrado['eventID'] == eventID]\n",
        "\n",
        "        # Ordenando por hitTime en orden descendente\n",
        "        df_evento_ordenado = df_evento.sort_values(by='hitTime', ascending=False)\n",
        "\n",
        "        # Ajustando el tamaño de los hits a N\n",
        "        num_hits = min(len(df_evento_ordenado), N)\n",
        "\n",
        "        # Inicializando listas para cada tipo de dato\n",
        "        hitX_values, hitY_values, hitZ_values, hitInteg_values = [], [], [], []\n",
        "\n",
        "        # Recolectando los valores separadamente\n",
        "        for i in range(num_hits):\n",
        "            hitX_values.append(df_evento_ordenado.iloc[i]['hitX'])\n",
        "            hitY_values.append(df_evento_ordenado.iloc[i]['hitY'])\n",
        "            hitZ_values.append(df_evento_ordenado.iloc[i]['hitZ'])\n",
        "            hitInteg_values.append(df_evento_ordenado.iloc[i]['hitInteg'])\n",
        "\n",
        "        # Concatenando los valores recolectados\n",
        "        hit_values = hitX_values + hitY_values + hitZ_values + hitInteg_values\n",
        "\n",
        "        # Rellenando con ceros si hay menos hits que N\n",
        "        if len(hit_values) < 4 * N:  # 4 tipos de datos * N hits\n",
        "            hit_values.extend([0] * (4 * N - len(hit_values)))\n",
        "\n",
        "        # Asegurarse de limitar a 4*N para considerar casos donde se excede\n",
        "        kaones.append(hit_values[:4 * N])\n",
        "        labels.append(df_evento.iloc[0]['PDGcode'])  # Tomar el PDGcode del primer hit como etiqueta\n",
        "\n",
        "    return np.array(kaones), np.array(labels)"
      ],
      "metadata": {
        "id": "lQ3QOngs00oV"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "N=60"
      ],
      "metadata": {
        "id": "YHI7ugPk1Jvz"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "matriz_train, etiquetas_train = filtrado_datos_concatenados(df_train, N)"
      ],
      "metadata": {
        "id": "OJg1adqu0jX6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_valid = 'df_valid.csv'\n",
        "path_valid = os.path.join(path_actual, subdirectorio, file_valid)\n",
        "\n",
        "df_valid = pd.read_csv(path_valid, index_col=None)\n",
        "df_valid = df_valid.drop(['trueE'],axis=1)\n"
      ],
      "metadata": {
        "id": "0jK6TgoL1QRU"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "matriz_valid, etiquetas_valid = filtrado_datos_concatenados(df_valid, N)"
      ],
      "metadata": {
        "id": "GC5kbnqS1lRm"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valores_unicos = np.unique(etiquetas_train)\n",
        "\n",
        "print(valores_unicos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "foD6ufIm9qdL",
        "outputId": "e0f950f8-43b4-42d7-c662-101610c19510"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[211. 321.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "valores_unicos_pdgcode = df_train['PDGcode'].unique()\n",
        "\n",
        "print(valores_unicos_pdgcode)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rDUZXPdq9_lv",
        "outputId": "76011483-7929-46e7-a631-881c8d559eb2"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[211 321]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Este modulo será el encargado de entrenar los modelos de machine learning y ver los resultados que muetran. Además elegiremos los mejores hiperparametros para los modelos ya que estos modelos serán cargados de la biblioteca Scikit-learn.\n",
        "\n",
        "Comprobaremos las diferencia de cada modelo para ver que modelo da mejores resultados."
      ],
      "metadata": {
        "id": "zjy8aJqy0YQx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Los modelos que vamos a usar son los siguientes:\n",
        "\n",
        "*   **Regresión logistica:** Es un modelo lineal que es simple pero eficaz para problemas de clasificación binaria. Este modelo nos dará el punto de partida.\n",
        "\n",
        "*   **Arboles de decisión:** Es un modelo versatil que puede usarse tanto para clasificación como para regresión. Los arboles de decisión son fáciles de interpretar y pueden capturar relaciones no lineales entre caracterisiticas.\n",
        "\n",
        "*   **Random Forest:** Un ensamble de arboles de decisión. Es más robusto y preciso que un árbol unico de decisión debido a que reduce el riesgo de sobreajuste mediante el promedio de múltiples árboles\n",
        "\n",
        "*   **SVM:** Es efectivo en espación de alta dimensión y casos donde el número de dimensiones es mayor que el número de muestras. Es útil tanto para la clasificación lineal como no lineal mediante el uso de diferentes kerneles.\n",
        "\n",
        "*   **GBM:** Es una técnica de ensamble que construye modelos de forma secuencial, cada nuevo modelo corrige errores del modelo anterior.\n",
        "\n",
        "*  **k-NN:** Es un modelo simple que clasifica puntos basándose en la mayoría de votos de sus k vecinos más cercanos. Es útil cuando las relaciones entre caracteristicas son complejas y no lineales.\n",
        "\n",
        "*  **XGBoost:** es una implementación optimizada de árboles de decisión aplicados al algoritmo de Gradient Boosting. Es muy eficaz por su eficiencia en la ejecución.\n",
        "\n",
        "*   **LightGBM:** es otro algoritmo de Gradient Boosting desarrollado por Microsoft que utiliza árboles basados en histogramas para el modelado.Utiliza la técnica de construcción de árboles basada en histogramas, que agrupa valores continuos en bins discretos\n",
        "\n"
      ],
      "metadata": {
        "id": "aTmzGgmbw5wa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "\n",
        "model = LogisticRegression()\n",
        "\n",
        "model.fit(matriz_train, etiquetas_train)\n",
        "\n",
        "\n",
        "\n",
        "predictions = model.predict(matriz_valid)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TnMNm_p5zwya",
        "outputId": "877bad12-a955-41e1-c877-0513a8530692"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcular el número de predicciones correctas\n",
        "correctas = np.sum(predictions == etiquetas_valid)\n",
        "\n",
        "# Calcular el porcentaje de precisión\n",
        "precision = (correctas / len(predictions)) * 100\n",
        "\n",
        "print(f'La precisión del modelo es: {precision}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-WZLTpeeF3JS",
        "outputId": "2fb957d6-ae09-4886-d71d-b4c60865ba92"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "La precisión del modelo es: 98.70422535211267%\n"
          ]
        }
      ]
    }
  ]
}