{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En esta sección elegiremos los mejores 4 modelos del notebook `modelosML.ipynb` para entrenarlos con Cross Validation. Para ello debemos de unir los datos de train con los de validacion, para que le modelo tenga más datos para entrenar y así poder generalizar mucho mejor. Los mejores modelos del notebook anterior son:\n",
    "*   **LGBM**\n",
    "*   **Random Forest**\n",
    "*   **GBC**\n",
    "*   **SVM**\n",
    "             "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación leeremos los datos de train y validacion para concatenarlos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Cambiar el directorio de trabajo\n",
    "ruta_nueva = 'C:/Users/ruben.morillas/Desktop/tfg/'  # Reemplaza 'TuUsuario' con tu nombre de usuario de Windows\n",
    "os.chdir(ruta_nueva)\n",
    "\n",
    "# Mostrar el directorio actual para confirmar el cambio\n",
    "directorio_actual = os.getcwd()\n",
    "print(\"Directorio actual:\", directorio_actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "path_actual = os.getcwd()\n",
    "subdirectorio = 'datas'\n",
    "file_train = 'df_train.csv'\n",
    "path_train = os.path.join(path_actual, subdirectorio, file_train)\n",
    "df_train = pd.read_csv(path_train, index_col=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_valid = 'df_valid.csv'\n",
    "path_valid = os.path.join(path_actual, subdirectorio, file_valid)\n",
    "df_valid = pd.read_csv(path_valid, index_col=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.drop(['trueE'], axis=1, inplace=True)\n",
    "df_valid.drop(['trueE'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combinado = pd.concat([df_train, df_valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtrado_datos(df, N):\n",
    "    kaones = []\n",
    "    labels = []\n",
    "\n",
    "    df_sorted = df.sort_values(by=['eventID', 'hitTime'], ascending=[True, False])\n",
    "\n",
    "    for eventID, grupo in df_sorted.groupby('eventID'):\n",
    "        pdgCodes = grupo['PDGcode'].unique()\n",
    "\n",
    "        for pdgCode in pdgCodes:\n",
    "            grupo_filtrado = grupo[grupo['PDGcode'] == pdgCode]\n",
    "            grupo_ordenado = grupo_filtrado.head(N)\n",
    "\n",
    "            # Inicializar arrays para el padding\n",
    "            hitX_padded = np.zeros(N)\n",
    "            hitY_padded = np.zeros(N)\n",
    "            hitZ_padded = np.zeros(N)\n",
    "            hitInteg_padded = np.zeros(N)\n",
    "\n",
    "            # Separar y aplicar padding a los valores de hitX, hitY, hitZ, hitInteg\n",
    "            hitX_padded[:len(grupo_ordenado['hitX'])] = grupo_ordenado['hitX']\n",
    "            hitY_padded[:len(grupo_ordenado['hitY'])] = grupo_ordenado['hitY']\n",
    "            hitZ_padded[:len(grupo_ordenado['hitZ'])] = grupo_ordenado['hitZ']\n",
    "            hitInteg_padded[:len(grupo_ordenado['hitInteg'])] = grupo_ordenado['hitInteg']\n",
    "\n",
    "            # Concatenar los valores ya con el padding aplicado\n",
    "            hit_values_reorganized = np.concatenate([hitX_padded, hitY_padded, hitZ_padded, hitInteg_padded])\n",
    "\n",
    "            kaones.append(hit_values_reorganized)\n",
    "\n",
    "            # Modificar las etiquetas de 211 a 0 y de 321 a 1\n",
    "            if pdgCode == 211:\n",
    "                labels.append(0)\n",
    "            elif pdgCode == 321:\n",
    "                labels.append(1)\n",
    "\n",
    "    return np.array(kaones), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = filtrado_datos(df_combinado, N=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importaremos las bibliotecas de los distintos modelos y también el de la CV, para poder entrenar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from lightgbm import LGBMClassifier # LightGBM\n",
    "from sklearn.svm import SVC # SVM\n",
    "from sklearn.ensemble import RandomForestClassifier # Random Forest\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RF = RandomForestClassifier(n_estimators=50, criterion='gini', max_depth=None, max_features='sqrt', random_state=42)\n",
    "scores = cross_val_score(RF, X, y, cv=5, scoring='accuracy')\n",
    "scores\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
